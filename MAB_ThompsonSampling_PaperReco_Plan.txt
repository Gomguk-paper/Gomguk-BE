### 핵심 아이디어 (Multi-Armed Bandit, Thompson Sampling)

- Arm(팔) = “추천 선택지”의 단위. (예: 태그/주제/추천전략)
- 각 Arm의 성공확률을 Beta 분포로 모델링하고, 요청마다 Beta에서 샘플링(Thompson Sampling)해 arm을 고른다.
- 모델 학습은 없음: 초기 prior(예: Beta(1,1))로 시작해 운영 중에만 업데이트한다.

---

### 요구사항 반영(중요): Paper DB → user.pi → (paper×tag) 매트릭스 → paper_id 정렬 → paper 조회

목표 플로우

1) 백엔드가 Paper DB에서 논문/태그 정보를 가져온다. (papers, paper_tags, tags)
2) models의 `user.pi`(유저 선호 벡터)를 참고해 “매트릭”(paper×tag incidence)을 이용해 각 paper의 점수를 만든다.
3) 사용자에 맞는 paper_id 리스트를 **점수 내림차순으로 정렬**한다.
4) 정렬된 paper_id로 다시 papers를 조회해 응답을 만든다. (정렬 유지)

이 문서에서는 `pi = {tag_id -> weight}`(사용자-태그 선호)로 정의하고,
`M[paper_id, tag_id] = 1(또는 가중치)`인 희소 매트릭스를 이용해 `score(paper)=Σ(pi[tag])`로 랭킹을 만든다.

---

### Arm 설계

Arm 정의: Tag 기반

- Arm = tag_id (app/models/paper.py의 Tag)
- 이유: 이미 Paper-Tag 관계가 있고, “논문 1개 = arm”처럼 폭발하지 않음.
- 사용자별 선호를 만들기 좋음: user_id + tag_id 단위로 TS 상태를 유지.
- (요구사항 연결) Tag-TS로 얻은 태그 가중치 벡터가 곧 `user.pi`가 되고, `paper×tag` 매트릭스로 paper_id를 정렬한다.

### Reward(보상) 정의

Thompson Sampling을 쓰려면 reward는 0/1(베르누이)로 두는 게 가장 단순하다.

권장(초기) 보상 정의: “긍정 행동 발생 여부”

- reward = 1: 사용자가 추천으로 노출된 논문을 “like 또는 scrap(save)” 함
- reward = 0: 일정 시간(예: 24시간) 내 like/scrap이 없었음 (또는 다음 피드 요청 시 미반응 처리)

보상 이벤트 소스(현 코드 기준)

- 이미 EventType에 like/save/view/search 등이 있음 (app/core/enums.py)
- 단, “추천 노출(impression)” 이벤트는 현재 enum에 없음.
    - create_event()는 EventType에 없는 문자열을 넣으면 스킵함(app/crud/event.py).
    - 따라서 추천 시스템 정확도를 위해선 아래 중 하나가 필요:
    (A) EventType에 impression/click 등을 추가하고 이벤트로 추적
    (B) 추천 전용 테이블(추천 노출/결과)을 별도로 만든다 (권장)

보상 강도(가중치) 옵션

- Beta 업데이트는 기본적으로 0/1이 깔끔하다.
- scrap을 like보다 더 강하게 치고 싶다면:
    - (권장) “목표를 2개로 분리”: like-bandit, scrap-bandit을 따로 운영

### 데이터/DB 설계 (권장 스키마)

3.1 사용자-태그 밴딧 상태 테이블

- 목적: user_id, tag_id에 대해 (alpha, beta)를 저장

예시 테이블: bandit_user_tag_stats

- user_id (FK [users.id](http://users.id/), index)
- tag_id (FK [tags.id](http://tags.id/), index)
- alpha (float or int, default 1)
- beta (float or int, default 1)
- updated_at (timestamp)
- UNIQUE(user_id, tag_id)

3.1.1 `user.pi` 저장/계산 방식(택1)

- (A) 런타임 계산(권장, 일관성 좋음)
    - 요청 시 `bandit_user_tag_stats`에서 (alpha,beta)를 가져와 theta 샘플/기댓값으로 `pi`를 즉석 생성
- (B) `users`에 캐시 저장(성능/단순화)
    - `users.meta['pi']`(JSONB)에 `{tag_id: weight}` 저장
    - 피드백/배치 시 갱신 (stale 허용)

---

3.2 추천 노출/결과 테이블 (failure를 계산하기 위해 필요)

- 목적: “추천을 보여줬는데 반응이 없었다”를 0 reward로 확정하기

예시 테이블: bandit_impressions

- id (PK)
- user_id
- tag_id (선택한 arm)
- paper_id (실제 노출된 논문)
- rec_id (UUID or ULID; FE에서 피드백 시 같이 보내기)
- shown_at
- resolved_at (nullable)
- outcome (enum/텍스트: pending/success/fail)
- UNIQUE(rec_id)

Failure 확정 방식(둘 중 하나)

- 방식 1) TTL 배치 처리
    - shown_at + TTL 지나면 outcome=pending → fail로 전환, beta += 1
    - 크론/워커 필요(간단한 스크립트라도 OK)
- 방식 2) “다음 피드 요청 시” 지연 확정(서버 내에서 처리)
    - get_feed 호출 시, pending 중 오래된 것들을 fail로 정리
    - 별도 배치 없이도 운영 가능(초기 MVP에 적합)

---

3.3 글로벌(전체 사용자) 태그 상태 (콜드스타트 완충)

- 목적: 신규 유저는 user-tag 데이터가 없어서 랜덤에 가까워질 수 있음
- 예시: bandit_global_tag_stats(tag_id, alpha, beta)
    - 또는 user_id=0 같은 “가상 유저”로 통일

### 추천 생성 로직 (Paper DB + user.pi + 매트릭스 기반 paper_id 랭킹)

입력

- user_id
- limit (기본 20)

출력

- 추천 논문 리스트 (PaperItem 등 기존 스키마 재사용)
- 각 아이템에 rec_id 포함(추후 피드백 연결용)

4.0 전제: Paper DB에서 “매트릭”을 만들 수 있어야 함

- Paper DB: `papers`, `paper_tags(paper_id, tag_id)`, `tags`
- 매트릭: `paper_tags`가 곧 희소 매트릭스 `M[paper, tag]`

4.1 후보 태그 집합 만들기 (`candidate_tag_ids`)

- 기본: 전체 tags
- 개선(성능/품질):
    - 사용자 히스토리 기반(좋아요/스크랩/조회한 논문의 태그) + 랜덤 태그 일부
    - 태그 count가 너무 작은 태그 제외(노이즈 제거)

4.2 Thompson Sampling으로 `user.pi` 만들기 (태그 가중치 벡터)

- 각 후보 tag에 대해 `theta[tag] ~ Beta(alpha, beta)` 샘플(또는 기댓값 `alpha/(alpha+beta)`)
- `pi[tag] = normalize(theta[tag])` 형태로 `user.pi` 구성
- 다양성/노이즈 제어:
    - top-k 태그만 남기고(k=20~50) 나머지는 0으로 두거나
    - 한 페이지에서 특정 tag의 노출 비율을 캡(예: tag당 최대 3개)

4.3 Paper DB에서 후보 paper를 뽑아 `paper_id`를 점수로 정렬

후보 paper 생성(성능을 위해 “전체 paper”가 아니라 subset을 권장)

- 후보 태그(top-k) 기준으로 `paper_tags` 조인해서 최근 논문 위주로 candidate paper_id 확보
- 필터:
    - 사용자가 이미 본/좋아요/스크랩한 paper 제외(또는 노출 로그 기준 제외)
    - 너무 오래된 논문 제외(예: 1~2년)
    - 품질/중복: 같은 출처/동일 저자/유사 제목 중복 제어(옵션)

점수 계산(매트릭 기반)

- paper의 tag 집합을 `T(paper)`라 하면:
    - `score(paper) = Σ_{t in T(paper)} pi[t]`
    - (옵션) 태그별 가중치(예: TF-IDF, tag.count 보정) 곱해도 됨

정렬 결과

- `ranked_paper_ids = sort_by(score desc, tie: published_at desc)`
- 응답은 `ranked_paper_ids[:limit]`를 기준으로 구성

4.4 paper_id로 papers를 다시 조회(정렬 유지) + 노출 기록 저장

- DB에서 `WHERE id IN (...)`으로 paper row를 가져오되, **ranked_paper_ids 순서를 유지**해야 함
    - 예: SQL `ORDER BY array_position(:ids, papers.id)` 또는 CASE WHEN 체인
- 추천 응답을 만들 때 `bandit_impressions`에 (user_id, tag_id(옵션), paper_id, rec_id, shown_at, outcome=pending) 저장
    - tag_id는 “주요 기여 태그(argmax pi[tag] among paper tags)”로 저장하면 분석에 유리

### 피드백/업데이트 로직

5.1 성공 처리(즉시)

- 사용자가 추천된 논문을 like/scrap하면:
    - 해당 paper_id의 가장 최근 pending impression(rec_id 기준이 가장 확실)을 success로 확정
    - bandit_user_tag_stats의 alpha += 1
    - (선택) bandit_global_tag_stats도 alpha += 1

5.2 실패 처리(지연)

- shown_at 이후 TTL 내 성공이 없으면:
    - outcome=pending → fail
    - beta += 1

5.3 같은 논문에 대한 중복 피드백 방지

- bandit_impressions.outcome이 이미 success/fail이면 추가 업데이트 금지
- (또는) UNIQUE(rec_id, feedback_type) 같은 장치

### API 설계 (현 엔드포인트와의 결합)

현재

- GET /paper/feed : “추천 시스템용”이라고 되어 있으나 현재는 최신순 피드(단순 정렬)

권장 MVP 변경(호환성 유지)

- GET /paper/feed?mode=ts
    - mode 미지정이면 기존 최신순 유지(롤백/디버그 쉬움)
    - mode=ts면 “user.pi + 매트릭 기반 paper_id 랭킹” 추천 피드 반환

피드백 API(권장)

- POST /paper/feed/feedback
    - body: { rec_id, action } (action: view/click/like/scrap 등)
    - 초기엔 like/scrap만 성공 처리(가장 단순)

view/click을 reward로 포함하고 싶다면

- “view/click = 1”로 두면 쉽지만, 품질이 떨어질 수 있음(낚시성 제목에 편향)
- 추천 품질이 중요하면 like/scrap 같은 ‘고의적 행동’만 reward로 쓰는 것을 권장

### 구현 위치(파일/모듈) 제안 (Gomguk-BE/backend)

모듈 분리(추천)

- app/services/bandit_thompson.py
    - select_tags_thompson()
    - pick_paper_for_tag()
    - resolve_pending_failures()
    - apply_feedback_success()
- app/models/bandit.py
    - BanditUserTagStat, BanditImpression, (optional) BanditGlobalTagStat
- app/api/routes/paper.py
    - get_feed(mode=ts) 분기 추가 또는 /recommendations 새 라우트 추가

트랜잭션/동시성

- alpha/beta 업데이트는 경쟁이 있을 수 있으므로 “행 잠금(SELECT ... FOR UPDATE)” 또는 “원자적 UPDATE” 고려
- 구현 난이도가 부담이면:
    - 우선 단일 UPDATE로도 충분(정확도 약간 손해)

### 운영/모니터링 지표

핵심 KPI

- Like rate / Scrap rate (추천 노출 대비)
- 세션당 positive action 수

품질/안정성

- Diversity: 태그 다양성(한 페이지/일 단위)
- Novelty: 신규 논문 비율, 미노출 논문 비율
- Cold start: 신규 유저의 첫 1~3일 성과

디버그

- 특정 유저의 user-tag alpha/beta, 최근 rec_id 목록 조회 API(내부용) 제공
1. 롤아웃/실행 순서 (실무 체크리스트)

Step 1) DB 스키마 추가

- bandit_user_tag_stats, bandit_impressions (+ optional global stats)
- Alembic revision 생성/적용

Step 2) 추천 응답에 rec_id 포함

- FE가 rec_id를 들고 있다가 피드백 시 그대로 전송할 수 있게

Step 3) 피드백 라우트 추가

- like/scrap 시 rec_id 기반 success 처리
- (초기) 실패는 다음 피드 요청 시 TTL로 정리

Step 4) feature flag

- 환경변수로 mode=ts on/off 가능하게(운영 안정성)

Step 5) 실험/튜닝

- prior 값(Beta(1,1) vs Beta(2,2))
- TTL(6h/24h/72h)
- 태그 후보 집합 제한 규칙(노이즈 제거)

### 간단한 의사코드 (Tag-TS)

추천 생성

1. resolve_pending_failures(user_id)
2. candidate_tags = get_candidate_tags(user_id)
3. user_pi = {}
   for tag in candidate_tags:
     (a,b) = get_or_init_user_tag_stat(user_id, tag)
     theta = sample_beta(a,b)  # 또는 mean
     user_pi[tag] = theta
   user_pi = normalize(user_pi)  # sum=1
4. candidate_paper_tag_rows = query_paper_tags_by_topk_tags(user_pi, k, recency_window)
5. scores = {}
   for (paper_id, tag_id) in candidate_paper_tag_rows:
     scores[paper_id] += user_pi.get(tag_id, 0)
6. ranked_paper_ids = sort(scores desc, tie: published_at desc)
7. papers = fetch_papers_by_ids_preserve_order(ranked_paper_ids[:limit])
8. for paper in papers:
     rec_id = new_uuid()
     insert impression(user_id, inferred_tag_id, paper.id, rec_id, pending)
9. 부족하면 fallback(최신순 등)으로 채우되, fallback은 별도 outcome 처리 규칙을 둔다.

성공 피드백

1. impression = find_by_rec_id(rec_id) and outcome=pending
2. mark impression outcome=success
3. stat.alpha += 1

실패 처리(지연)

1. pending impressions where shown_at < now - TTL
2. mark outcome=fail
3. stat.beta += 1
